{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import urllib\n",
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapes all hrefs on a page. Returns a set to prevent duplicate hrefs.\n",
    "def get_hrefs(url):\n",
    "    r = requests.get(url)\n",
    "    bsObj = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "    final_links = {link.get(\"href\") for link in bsObj.find_all(\"a\")}\n",
    "\n",
    "    return final_links\n",
    "\n",
    "def decode_href(href):\n",
    "    decoded = bytes(href, \"utf-8\").decode(\"unicode_escape\")\n",
    "    # Replace escaped slashes and remove begin and end quotes.\n",
    "    decoded = decoded.replace(\"\\\\/\", \"/\")[1:-1]\n",
    "    # Some vergaderingen are duplicated due to this anchor tag,\n",
    "    # I remove the tag so the duplicate vergadering is filtered.\n",
    "    decoded = decoded.split(\"#\")[0]\n",
    "\n",
    "    return decoded\n",
    "\n",
    "\n",
    "def save_hrefs(hrefs, year):\n",
    "    if not os.path.isdir(f\"data/ridderkerk/{year}\"):\n",
    "        os.mkdir(f\"data/ridderkerk/{year}\")\n",
    "\n",
    "    with open(f\"data/ridderkerk/{year}/vergaderingen.txt\", \"w\") as vergaderingen_f:\n",
    "        with open(f\"data/ridderkerk/{year}/documenten.txt\", \"w\") as documenten_f:\n",
    "            with open(f\"data/ridderkerk/{year}/modules.txt\", \"w\") as modules_f:\n",
    "                with open(f\"data/ridderkerk/{year}/rest.txt\", \"w\") as rest_f:\n",
    "                    for href in hrefs:\n",
    "                        if \"vergadering\" in href:\n",
    "                            vergaderingen_f.write(href + \"\\n\")\n",
    "                        elif \"document\" in href:\n",
    "                            documenten_f.write(href + \"\\n\")\n",
    "                        elif \"module\" in href:\n",
    "                            modules_f.write(href + \"\\n\")\n",
    "                        else:\n",
    "                            rest_f.write(href + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "if not os.path.isdir(\"data/ridderkerk\"):\n",
    "    os.mkdir(\"data/ridderkerk\")\n",
    "\n",
    "years = range(2010, 2025)\n",
    "for year in years:\n",
    "    if (\n",
    "        os.path.isdir(f\"data/ridderkerk/{year}/vergaderingen.txt\")\n",
    "        or os.path.isdir(f\"data/ridderkerk/{year}/documenten.txt\")\n",
    "        or os.path.isdir(f\"data/ridderkerk/{year}/modules.txt\")\n",
    "        or os.path.isdir(f\"data/ridderkerk/{year}/rest.txt\")\n",
    "    ):\n",
    "        print(f\"year {year} already scraped.\")\n",
    "        continue\n",
    "\n",
    "    all_hrefs_year = set()\n",
    "    for page in range(1, 30):\n",
    "        BASE_URL = f\"https://ridderkerk.notubiz.nl/zoeken/result?keywords=vergadering&limit=25&document_type=&search=send&filter[organisations][]=353&page={page}&filter[date][]={year}\"\n",
    "        all_hrefs_year.update(get_hrefs(BASE_URL))\n",
    "\n",
    "    # Decode strings and remove duplicates.\n",
    "    all_hrefs_year = set(list(map(decode_href, all_hrefs_year)))\n",
    "\n",
    "    print(f\"Got data for {year}, total links: {len(all_hrefs_year)}\")\n",
    "    save_hrefs(all_hrefs_year, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_vergadering(url, name):\n",
    "    # r = requests.get(url)\n",
    "    # For some reason request.get gave me 500, urllib works fine.\n",
    "    try:\n",
    "        r = urllib.request.urlopen(url)\n",
    "    except Exception as e:\n",
    "        return False\n",
    "    # print(r.status_code, r.reason)\n",
    "    bsObj = BeautifulSoup(r, \"html.parser\")\n",
    "    download_url = bsObj.find(href=re.compile(\"download\"))\n",
    "\n",
    "    if not download_url:\n",
    "        return False\n",
    "    download_url = download_url.get(\"href\")\n",
    "\n",
    "    try:\n",
    "        r = requests.get(download_url, stream=True)\n",
    "        print(f\"Downloading {download_url}\")\n",
    "        with open(f\"data/ridderkerk/{year}/videos/{name}\", \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    except Exception as _:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    if not os.path.isdir(f\"data/ridderkerk/{year}/videos\"):\n",
    "        os.mkdir(f\"data/ridderkerk/{year}/videos\")\n",
    "    with open(f\"data/ridderkerk/{year}/vergaderingen.txt\", \"r\") as vergaderingen_f:\n",
    "        for url in vergaderingen_f:\n",
    "            url = url.replace(\"\\n\", \"\")\n",
    "            code = url.replace(\"\\n\", \"\").split(\"/\")[-1]\n",
    "            # extension = download_url.split(\".\")[-1]\n",
    "            extension = \"mp4\"\n",
    "            name = f\"{code}.{extension}\"\n",
    "\n",
    "            if (\n",
    "                os.path.isfile(f\"data/ridderkerk/{year}/videos/{name}\")\n",
    "                or os.path.isfile(f\"data/ridderkerk/{year}/audio/{name}.mp4\")\n",
    "                or os.path.isfile(f\"data/ridderkerk/{year}/audio/{name}\")\n",
    "            ):\n",
    "                print(f\"{url} already downloaded.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Trying {url}\")\n",
    "            if not download_vergadering(url, name):\n",
    "                with open(f\"data/ridderkerk/{year}/failed.txt\", \"a+\") as f:\n",
    "                    f.write(url + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    if not os.path.isdir(f\"data/ridderkerk/{year}/agendas\"):\n",
    "        os.mkdir(f\"data/ridderkerk/{year}/agendas\")\n",
    "    with open(\n",
    "        f\"data/ridderkerk/{year}/vergaderingen.txt\", \"r\"\n",
    "    ) as vergaderingen_f:\n",
    "        for url in vergaderingen_f:\n",
    "            url = url.replace(\"\\n\", \"\")\n",
    "            code = url.replace(\"\\n\", \"\").split(\"/\")[-1]\n",
    "            output_path = (\n",
    "                f\"data/ridderkerk/{year}/agendas/{code}.json\"\n",
    "            )\n",
    "            if os.path.isfile(output_path):\n",
    "                print(f\"{output_path} already exists.\")\n",
    "                continue\n",
    "\n",
    "            vergadering_json = []\n",
    "            r = requests.get(url)\n",
    "            bsObj = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "            agenda_items = [li for li in bsObj.find_all(\"li\", class_=\"agenda_item\")]\n",
    "            # print(agenda_items)\n",
    "            for item in agenda_items:\n",
    "                agenda_point = {}\n",
    "                btn = item.find(\"button\", class_=\"item_title\")\n",
    "                if not btn:\n",
    "                    continue\n",
    "                span = btn.find(\"span\", class_=\"item_prefix\")\n",
    "\n",
    "                # Skip sub agenda item (for now, perhaps)\n",
    "                if not span or not span.text.strip().endswith(\".\"):\n",
    "                    continue\n",
    "\n",
    "                agenda = btn.get_text(strip=True)\n",
    "                agenda_point[\"agendaPoint\"] = agenda\n",
    "                time_span = item.find(\"span\", class_=\"item_time\")\n",
    "                if time_span:\n",
    "                    time = time_span.text.strip().replace(\"tijdsduur:\", \"\").strip()\n",
    "                    agenda_point[\"time\"] = time\n",
    "\n",
    "                vergadering_json.append(agenda_point)\n",
    "\n",
    "            with open(output_path, \"w\") as f:\n",
    "                json.dump(vergadering_json, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
